{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "\n",
    "Von  Jannis Breitenstein, Jonas Meyer, Verena Aschoff, Lucas Grewe, Thomas Wilmes <br>\n",
    "\n",
    "\n",
    "Diese Notebook ist in folgende Abschnitte unterteilt:\n",
    "\n",
    "+ [1. Import Bibliotheken](#1)<br>\n",
    "+ [2. Laden und Aufbereiten der Filmdaten](#2)<br>\n",
    "+ [2.1 Laden und Aufbereitung der Dten](#2.1)<br>\n",
    "+ [2.2 Exportieren und Einlesen der DAten](#2.1)<br>\n",
    "+ [3. Explorative Datenanalyse](#3)<br>\n",
    "+ [3.1 Allgeime Daten über die Datensets](#3.1)<br>\n",
    "+ [3.2 Veröffentlichung der Filme](#3.2)<br>\n",
    "+ [3.3 Verteilung der Filmbewertungen](#3.3)<br>\n",
    "+ [3.4 Wann sind die Filme bewertet worden](#3.4)<br>\n",
    "+ [3.5 Verteilung von Filmbewertungen und Benutzern](#3.5)<br>\n",
    "+ [4. Filtern der Daten](#4)<br>\n",
    "+ [5. Anweden des KNN-Modells](#5)<br>\n",
    "+ [5.1 Vorbereitung des KNN-Modells](#5.1)<br>\n",
    "+ [5.2 Berechnung von allen Nachbarn und Export](#5.2)<br>\n",
    "+ [5.3 Vorbereitung für Precision and Recall](#5.3)<br>\n",
    "+ [5.4 Presicion and Recall](#5.4)<br>\n",
    "+ [5.5 Empfehlung mit Hilfe des Fuzzy Algorithmus](#5.5)<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=1>1. Import Bibliotheken </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Package\n",
    "# Alternativ die Pakete installieren\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "## Es wird empfohlen, das Jupyter Notebook Paket zu installieren\n",
    "!{sys.executable} -m pip install jupyter\n",
    "###############################################\n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install surprise\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pytz\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install scikit-surprise \n",
    "\n",
    "# Wird benötigt, wenn man dieses Notebook über Colabs benutzen möchte\n",
    "#!{sys.executable} -m pip install google.colab\n",
    "###############################################\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install plotly\n",
    "!{sys.executable} -m pip install fastparquet\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install pyarrow\n",
    "!{sys.executable} -m pip install fuzzywuzzy\n",
    "!{sys.executable} -m pip install python-Levenshtein\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zum Speichern der Daten\n",
    "import pandas as pd\n",
    "\n",
    "# Wird zur Erstellung mehrdimensionalen Arrays benötigt\n",
    "import numpy as np\n",
    "\n",
    "# interaktive Diagramme erstellen\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# interaktive Diagramme erstellen\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Zum Verschieben von Listen\n",
    "from collections import deque\n",
    "\n",
    "# Ähnlichkeiten zwischen Vektoren zu berechnen\n",
    "from sklearn.neighbors import NearestNeighbors , BallTree, KDTree\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# TO Fuzz\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Zum erstellen von Matrizen\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import time\n",
    "# Runtime variable\n",
    "import json\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <a id=2>2. Laden und Aufbereiten der Filmdaten </a>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Datenaufbereitung </a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definieren der Dateipfade\n",
    "\n",
    "# Enhält alle Filme mit Namen, Jahr und Film-ID.\n",
    "# Ändern der Dateipfade für den entsprechenden Ordner.\n",
    "\n",
    "combined_data_1 = './combined_data_1.txt'\n",
    "combined_data_2 = './combined_data_2.txt'\n",
    "combined_data_3 = './combined_data_3.txt'\n",
    "combined_data_4 = './combined_data_4.txt'\n",
    "\n",
    "#Auslesen der Daten aus den CSV-Dateien und mit neue Überschriften vergeben\n",
    "\n",
    "combined_data_1_raw = pd.read_csv(combined_data_1, header=None, names=['Cust_Id', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "combined_data_2_raw = pd.read_csv(combined_data_2, header=None, names=['Cust_Id', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "combined_data_3_raw = pd.read_csv(combined_data_3, header=None, names=['Cust_Id', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "combined_data_4_raw = pd.read_csv(combined_data_4, header=None, names=['Cust_Id', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "\n",
    "## Aufgrunddessen, dass die Combined Data nicht nicht einfach verarbeitet können, müssen die Daten erst ausgelesen und als neue Datei gespeichert werden,\n",
    "#  um in Zukunft einfacher auf die Daten zugreifen zu können.\n",
    "\n",
    "# Suche nach leeren Zeilen, um das Dataframe für jeden Film aufzuschneiden\n",
    "tmp_movies = combined_data_1_raw[combined_data_1_raw['Rating'].isna()]['Cust_Id'].reset_index()  \n",
    "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
    "\n",
    "# Verschiebe die movie_indices um eins, um Start- und Endpunkte aller Filme zu erhalten\n",
    "shifted_movie_indices = deque(movie_indices)\n",
    "shifted_movie_indices.rotate(-1)\n",
    "\n",
    "\n",
    "# Sammeln aller Dataframes\n",
    "user_data = []\n",
    "\n",
    "# Iterieren über alle Filme\n",
    "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "    \n",
    "    # Prüfen, ob es der letzte Film in der Datei ist\n",
    "    if df_id_1<df_id_2:\n",
    "        tmp_df = combined_data_1_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "    else:\n",
    "        tmp_df = combined_data_1_raw.loc[df_id_1+1:].copy()\n",
    "        \n",
    "    # Spalte Movie_id erstellen\n",
    "    tmp_df['Movie_Id'] = movie_id\n",
    "    \n",
    "    # Datenrahmen an die Liste anhängen\n",
    "    user_data.append(tmp_df)\n",
    "\n",
    "# Alle Dataframes zusammenfassen\n",
    "df_1 = pd.concat(user_data)\n",
    "del user_data, combined_data_1_raw, combined_data_1, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
    "\n",
    "### Dieses Vorgehen wird für die anderen Dataframes wiederholt.\n",
    "\n",
    "tmp_movies = combined_data_2_raw[combined_data_2_raw['Rating'].isna()]['Cust_Id'].reset_index() \n",
    "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
    "\n",
    "shifted_movie_indices = deque(movie_indices)\n",
    "shifted_movie_indices.rotate(-1)\n",
    "\n",
    "user_data = []\n",
    "\n",
    "\n",
    "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "    \n",
    "    if df_id_1<df_id_2:\n",
    "        tmp_df = combined_data_2_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "    else:\n",
    "        tmp_df = combined_data_2_raw.loc[df_id_1+1:].copy()\n",
    "        \n",
    "    tmp_df['Movie_Id'] = movie_id\n",
    "    \n",
    "    user_data.append(tmp_df)\n",
    "\n",
    "df_2 = pd.concat(user_data)\n",
    "del user_data, combined_data_2_raw, combined_data_2, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
    "\n",
    "\n",
    "tmp_movies = combined_data_3_raw[combined_data_3_raw['Rating'].isna()]['Cust_Id'].reset_index() \n",
    "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
    "\n",
    "shifted_movie_indices = deque(movie_indices)\n",
    "shifted_movie_indices.rotate(-1)\n",
    "\n",
    "user_data = []\n",
    "\n",
    "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "    \n",
    "    if df_id_1<df_id_2:\n",
    "        tmp_df = combined_data_3_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "    else:\n",
    "        tmp_df = combined_data_3_raw.loc[df_id_1+1:].copy()\n",
    "        \n",
    "    tmp_df['Movie_Id'] = movie_id\n",
    "    \n",
    "    user_data.append(tmp_df)\n",
    "\n",
    "\n",
    "df_3 = pd.concat(user_data)\n",
    "del user_data, combined_data_3_raw, combined_data_3, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
    "\n",
    "tmp_movies = combined_data_4_raw[combined_data_4_raw['Rating'].isna()]['Cust_Id'].reset_index()\n",
    "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
    "\n",
    "\n",
    "shifted_movie_indices = deque(movie_indices)\n",
    "shifted_movie_indices.rotate(-1)\n",
    "\n",
    "user_data = []\n",
    "\n",
    "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "    \n",
    "    if df_id_1<df_id_2:\n",
    "        tmp_df = combined_data_4_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "    else:\n",
    "        tmp_df = combined_data_4_raw.loc[df_id_1+1:].copy()\n",
    "        \n",
    "    tmp_df['Movie_Id'] = movie_id\n",
    "    \n",
    "    user_data.append(tmp_df)\n",
    "\n",
    "df_4 = pd.concat(user_data)\n",
    "del user_data, combined_data_4_raw, combined_data_4, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
    "print('Shape User-Ratings:\\t{}'.format(df_4.shape))\n",
    "\n",
    "#Zusammenfügen der aller Daten in einer Variable\n",
    "data = [df_1, df_2,df_3,df_4]\n",
    "df_rating = pd.concat(data)\n",
    "\n",
    "# Löschen der Variablen\n",
    "del df_1, df_2, df_3, df_4, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der nächste Schritt ist es, die Movie-Titel Datei zu importieren.\n",
    "# Aufgrunddessen, dass Filme aus Seperatoren wie , oder ; enhalten, werden zunächst alle Daten in eine Spalte geladen und am Ende separiert. \n",
    "# Ändern der Dateipfade für den entsprechenden Ordner.\n",
    "\n",
    "movie_tile_File = './movie_titles.csv'\n",
    "########################################\n",
    "df_movie_titles = pd.read_csv(movie_tile_File,\n",
    "                           encoding = \"ISO-8859-1\",\n",
    "                           delimiter= '\\t',\n",
    "                           header = None,\n",
    "                           names = ['Ziel'])\n",
    "                           # Speicher alle Daten erst in eine Spalte, danach trennt er diese\n",
    "df_movie_titles[['Movie_Id', 'Year', 'Name']] = df_movie_titles['Ziel'].str.split(pat=\",\",n=2, expand=True)   \n",
    "\n",
    "# Entfernen der Spalte Ziel\n",
    "df_movie_titles= df_movie_titles.drop(['Ziel'], axis= 1)  \n",
    "\n",
    "# Speichern des Dataframes in der entsprechenden Formatierung\n",
    "Convert_dic= {'Movie_Id': 'int64','Year': 'object', 'Name': 'str'}\n",
    "df_movie_titles = df_movie_titles.astype(Convert_dic)\n",
    "\n",
    "\n",
    "del Convert_dic, movie_tile_File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.2 Exportieren und Einlesen der Daten </a> \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damit in Zukunft die Iteration nicht immer erneut Ausgeführt werden soll, werden die Daten in eine Parquet Datei geschrieben.\n",
    "# Ändern der Dateipfade für den entsprechenden Ordner.\n",
    "parquet_combined_data = './data_Comb.zip'\n",
    "#########################################\n",
    "\n",
    "df_rating.to_parquet(parquet_combined_data, index=False)\n",
    "del parquet_combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einlesen der zuvor erstellen Parquet Datei  --> Rating-User-Movie_Id.\n",
    "# Ändern der Dateipfade für den entsprechenden Ordner.\n",
    "parquet_combined_data = './data_Comb.zip'\n",
    "#############################################\n",
    "\n",
    "df_rating = pd.read_parquet(parquet_combined_data)\n",
    "# Formatieren des Dateiformats\n",
    "Convert_dic_data= {'Cust_Id': 'int64', 'Rating': 'float32','Date': 'str', 'Movie_Id': 'Int64'}\n",
    "df_rating= df_rating.astype(Convert_dic_data)\n",
    "del parquet_combined_data, Convert_dic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damit in Zukunft das Einlesen und das Trennen der Spalten nicht immer erneut ausgeführt werden soll, werden die Daten in eine Parquet Datei geschrieben.\n",
    "# Ändern der Dateipfade für den entsprechenden Ordner.\n",
    "parquet_movietitles_data = './movie_titles.zip'\n",
    "##############################################\n",
    "df_movie_titles.to_parquet(parquet_movietitles_data, index=False)\n",
    "del parquet_movietitles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einlesen der zuvor erstellen Parquet Datei.\n",
    "# Ändern der Dateipfade für den entsprechenden Ordner.\n",
    "parquet_movietitles_data = './movie_titles.zip'\n",
    "#####################################################\n",
    "\n",
    "df_movie_titles = pd.read_parquet(parquet_movietitles_data)\n",
    "# Formatieren des Dateiformats\n",
    "Convert_dic= {'Movie_Id': 'int64', 'Year': 'object', 'Name': 'str'}\n",
    "df_movie_titles= df_movie_titles.astype(Convert_dic)\n",
    "del parquet_movietitles_data, Convert_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "# <a id=3>3. Explorative Datenanalyse </a>\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Allgeime Daten über die Datensets </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenstruktur Filmbewertungen\n",
    "\n",
    "display(df_rating)\n",
    "\n",
    "#Anzahl der Benutzer\n",
    "num_users = len(df_rating.Cust_Id.unique())\n",
    "\n",
    "#Anzahl der eindeutigen Film-IDs\n",
    "num_items = len(df_rating.Movie_Id.unique())\n",
    "print('Es gibt {} eindeutige Users und {} eindeutige Filme im Dataframe'.format(num_users, num_items))\n",
    "\n",
    "# Anzahl der Rating bei Gruppe\n",
    "df_ratings_cnt_tmp = pd.DataFrame(df_rating.groupby('Rating').size(), columns=['count'])\n",
    "df_ratings_cnt_tmp\n",
    "\n",
    "\n",
    "# Anzahl der von jedem Nutzer abgegebenen Bewertung\n",
    "df_users_cnt = pd.DataFrame(df_rating.groupby('Cust_Id').size(), columns=['count'])\n",
    "df_users_cnt.head()\n",
    "\n",
    "\n",
    "##  Ausgabe der Duplikate\n",
    "dupli = df_rating.duplicated(keep=False).sum()\n",
    "print('In dem Datensatz df_Rating liegen {} Duplikate vor'.format(dupli))\n",
    "\n",
    "# Ausgabe der Datei Filmdaten\n",
    "\n",
    "display(df_movie_titles)\n",
    "\n",
    "del num_items, num_users, df_ratings_cnt_tmp, df_users_cnt, dupli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Veröffentlichung der Filme</a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einholen\n",
    "data = df_movie_titles['Year'].value_counts().sort_index()\n",
    "\n",
    "# Linie erstellen\n",
    "trace = go.Scatter(x = data.index,\n",
    "                   y = data.values,\n",
    "                   marker = dict(color = '#db0000'))\n",
    "# Layout erstellen\n",
    "layout = dict(title = '{} Filme gruppiert nach Jahr der Veröffentlichung'.format(df_movie_titles.shape[0]),\n",
    "              xaxis = dict(title = 'Erscheinungsjahr'),\n",
    "              yaxis = dict(title = 'Filme'))\n",
    "\n",
    "# Plot erstellen\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "del layout, trace, fig, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.3. Bewertung der Filme</a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einholen\n",
    "data = df_rating['Rating'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "# Linie erstellen\n",
    "trace = go.Bar(x = data.index,\n",
    "               text = ['{:.1f} %'.format(val) for val in (data.values / df_rating.shape[0] * 100)],\n",
    "               textposition = 'auto',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               y = data.values,\n",
    "               marker = dict(color = '#db0000'))\n",
    "#  Layout erstellen\n",
    "layout = dict(title = 'Verteilung {} User-Ratings'.format(df_rating.shape[0]),\n",
    "              xaxis = dict(title = 'Rating'),\n",
    "              yaxis = dict(title = 'Anzahl'))\n",
    "# Plot erstellen\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "## Variablen löschen\n",
    "del fig, layout, data, trace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.4. Wann sind die Filme bewertet worden?</a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einholen\n",
    "data = df_rating['Date'].value_counts()\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "# Linie erstellen\n",
    "trace = go.Scatter(x = data.index,\n",
    "                   y = data.values,\n",
    "                   marker = dict(color = '#db0000'))\n",
    "# Layout erstellen\n",
    "layout = dict(title = '{} Movie-Rating gruppiert mit dem Attribut \"Date\"'.format(df_rating.shape[0]),\n",
    "              xaxis = dict(title = 'Datum'),\n",
    "              yaxis = dict(title = 'Ratings'))\n",
    "\n",
    "# Plot erstellen\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "## Delete Variables\n",
    "del fig, layout, data, trace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.5. Verteilung von Filmbewertungen und Benutzern </a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bewertungen je Film #####\n",
    "# Daten einholen\n",
    "data = df_rating.groupby('Movie_Id')['Rating'].count().clip(upper=9999)\n",
    "\n",
    "# Linie erstellen\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 10000,\n",
    "                                  size = 100),\n",
    "                     marker = dict(color = '#db0000'))\n",
    "# Layout erstellen\n",
    "layout = go.Layout(title = 'Distribution Of Ratings Per Movie (Clipped at 9999)',\n",
    "                   xaxis = dict(title = 'Ratings Per Movie'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Plot erstellen\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "del fig, layout, data, trace\n",
    "\n",
    "\n",
    "##### Ratings je User #####\n",
    "# Daten einholen\n",
    "data = df_rating.groupby('Cust_Id')['Rating'].count().clip(upper=299)\n",
    "\n",
    "# Linie erstellen\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 1000,\n",
    "                                  size = 2),\n",
    "                     marker = dict(color = '#db0000'))\n",
    "# Layout erstellen\n",
    "layout = go.Layout(title = 'Distribution Of Ratings Per User (Clipped at 199)',\n",
    "                   xaxis = dict(title = 'Ratings Per User'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Plit erstelle\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "## Delete Variables\n",
    "del fig, layout, data,  trace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative Darstellung\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "data = df_rating.groupby('Cust_Id')['Rating'].count()\n",
    "sns.distplot(data[data  < 200], kde=False, ax=ax[0]);\n",
    "sns.distplot(data[data  > 200], kde=False, ax=ax[1], bins=[i for i in range(100,1000,20)]);\n",
    "\n",
    "del fig, ax, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Alternative Darstellung\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "data = df_rating.groupby('Movie_Id')['Rating'].count()\n",
    "\n",
    "\n",
    "sns.histplot(data[data  < 10000], kde=False, ax=ax[0], bins=[i for i in range(0,10000,200)]);\n",
    "sns.histplot(data[data  > 10000], kde=False, ax=ax[1], bins=[i for i in range(10000,150000,5000)]);\n",
    "\n",
    "\n",
    "del fig, ax, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <a id=4> 4. Filtern der Daten</a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtern von Mindestanzahl von Ratings\n",
    "# Derzeit werden keine Filme gefiltert.\n",
    "min_movie_ratings = 0\n",
    "filter_movies = (df_rating['Movie_Id'].value_counts()>min_movie_ratings)\n",
    "filter_movies = filter_movies[filter_movies].index.tolist()\n",
    "\n",
    "# Filtern von Usern die weniger als min_user_ratings bewertet haben\n",
    "min_user_ratings = 400\n",
    "filter_users = (df_rating['Cust_Id'].value_counts()>min_user_ratings)\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "# Actual filtering\n",
    "df_filterd = df_rating[(df_rating['Movie_Id'].isin(filter_movies)) & (df_rating['Cust_Id'].isin(filter_users))]\n",
    "\n",
    "print('Shape User-Ratings ungefiltert:\\t{}'.format(df_rating.shape))\n",
    "print('Shape User-Ratings gefiltert:\\t{}'.format(df_filterd.shape))\n",
    "\n",
    "# Entfernen der Spalte Datum aus \n",
    "df_filterd = df_filterd.drop('Date', axis= 1)\n",
    "\n",
    "# Löschen der Variablen\n",
    "del filter_movies, filter_users, min_movie_ratings, min_user_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <a id=5>5. Anwenden des KNN-Modells</a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.1 Vorbereitungen für das KNN-Modell</a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen der Spalte Datum aus \n",
    "df_filterd = df_filterd.drop('Date', axis= 1)\n",
    "\n",
    "# Erstellen eines Film-Recommendation Dataframe und den Distance\n",
    "df_recommendation = pd.DataFrame(columns=['Movie_Id', 'R_Movie_Id_1', 'R_Distance_1' , 'R_Movie_Id_2'  , 'R_Distance_2' , 'R_Movie_Id_3'  ,'R_Distance_3','R_Movie_Id_4' ,'R_Distance_4' ,'R_Movie_Id_5' ,'R_Distance_5','R_Movie_Id_6' ,'R_Distance_6'])\n",
    "Convert_dic= {'Movie_Id': 'int32', 'R_Movie_Id_1': 'int32','R_Distance_1':'float','R_Movie_Id_2': 'int32','R_Distance_2':'float','R_Movie_Id_3': 'int32','R_Distance_3':'float','R_Movie_Id_4': 'int32','R_Distance_4':'float','R_Movie_Id_5': 'int32','R_Distance_5':'float', 'R_Movie_Id_6': 'int32','R_Distance_6':'float'}\n",
    "df_recommendation = df_recommendation.astype(Convert_dic)\n",
    "\n",
    "# Löschen der Variablen\n",
    "del Convert_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.1.1 Bruce Model</a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Pivotmatrix movie-user, alle nicht vorhandenen Werte werden mit Null aufgeügllt\n",
    "movie_user_mat = df_filterd.pivot(index='Movie_Id', columns='Cust_Id', values='Rating').fillna(0)\n",
    "\n",
    "\n",
    "# Übergeben der Parameter für KNN\n",
    "user_movie_table_matrix = csr_matrix(movie_user_mat)\n",
    "\n",
    "# Übergeben der Parameter für KNN\n",
    "model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute',n_jobs=-1, n_neighbors= 7)\n",
    "model_knn.fit(user_movie_table_matrix)\n",
    "\n",
    "### Erstellen der Arrays Distance (Metrik) und Distanz (Nachbar)\n",
    "distances, indices = model_knn.kneighbors(user_movie_table_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Index um 1 erhöhen und Exportieren\n",
    "\n",
    "Dateipfad = './Nearest_Neighbor.csv'\n",
    "pd.DataFrame(indices).to_csv(Dateipfad, index=False, header=False)\n",
    "df_nachbarn = pd.read_csv(Dateipfad, names = ['ID', 'n_1','n_2','n_3','n_4','n_5','n_6'])\n",
    "#Erhöht jeden Wert in der Spalte um 1, muss angepasst werden, wenn die n_neighbors angepasst werden.\n",
    "for index, row in df_nachbarn.iterrows():\n",
    "\n",
    "    row['ID']+=1\n",
    "    row['n_1']+=1\n",
    "    row['n_2']+=1\n",
    "    row['n_3']+=1\n",
    "    row['n_4']+=1\n",
    "    row['n_5']+=1\n",
    "    row['n_6']+=1\n",
    "\n",
    "pd.DataFrame(df_nachbarn).to_csv('./Nearest_Neighbor+1.csv', sep=',', index=False, header=True)\n",
    "\n",
    "del  row, index, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.2 Export der nächsten Nacharn</a>\n",
    "***\n",
    "Dieser Abschnitt kann ignoriert werden und dient nur der Aufbereitung und Verarbeitung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen eines leeren Dataframes\n",
    "\n",
    "df_recommendation = pd.DataFrame(columns=['Movie_Id', 'R_Movie_Id_1', 'R_Distance_1' , 'R_Movie_Id_2'  , 'R_Distance_2' , 'R_Movie_Id_3'  ,'R_Distance_3','R_Movie_Id_4' ,'R_Distance_4' ,'R_Movie_Id_5' ,'R_Distance_5','R_Movie_Id_6' ,'R_Distance_6'])\n",
    "Convert_dic= {'Movie_Id': 'int32', 'R_Movie_Id_1': 'int32','R_Distance_1':'float','R_Movie_Id_2': 'int32','R_Distance_2':'float','R_Movie_Id_3': 'int32','R_Distance_3':'float','R_Movie_Id_4': 'int32','R_Distance_4':'float','R_Movie_Id_5': 'int32','R_Distance_5':'float', 'R_Movie_Id_6': 'int32','R_Distance_6':'float'}\n",
    "df_recommendation = df_recommendation.astype(Convert_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen einer Liste mit allen vorhandenen Movie_IDs in der Movie-User Matrix, diese dient als\n",
    "movie_index_list = list(movie_user_mat.index.values)\n",
    "\n",
    "### Durch das Iterieren der Daten werden 6 Empfehlungen in ein Dataframe geschrieben.\n",
    "for  x in movie_index_list:\n",
    "\n",
    "    movie = []\n",
    "    distance = []\n",
    "        \n",
    "\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i != 0:\n",
    "            movie.append(movie_user_mat.index[indices.flatten()[i]])\n",
    "            distance.append(distances.flatten()[i])    \n",
    "\n",
    "    m=pd.Series(movie,name='Movie_Id')\n",
    "    d=pd.Series(distance,name='distance')\n",
    "    recommend = pd.concat([m,d], axis=1)\n",
    "    recommend = recommend.sort_values('distance',ascending=False)\n",
    "\n",
    "    # Anfügen der Empfehlungen für jeden Film in \n",
    "    df_recommendation = df_recommendation.append({'Movie_Id': movie_id, 'R_Movie_Id_1': recommend['Movie_Id'].iloc[0], 'R_Distance_1':recommend['distance'].iloc[0],\n",
    "        'R_Movie_Id_2': recommend['Movie_Id'].iloc[1], 'R_Distance_2':recommend['distance'].iloc[1],\n",
    "        'R_Movie_Id_3': recommend['Movie_Id'].iloc[2], 'R_Distance_3':recommend['distance'].iloc[2],\n",
    "        'R_Movie_Id_4': recommend['Movie_Id'].iloc[3], 'R_Distance_4':recommend['distance'].iloc[3],\n",
    "        'R_Movie_Id_5': recommend['Movie_Id'].iloc[4], 'R_Distance_5':recommend['distance'].iloc[4],\n",
    "        'R_Movie_Id_6': recommend['Movie_Id'].iloc[5], 'R_Distance_6':recommend['distance'].iloc[5]\n",
    "        },ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nachbarn = df_recommendation.drop(['R_Distance_1', 'R_Distance_2', 'R_Distance_3', 'R_Distance_4', 'R_Distance_5', 'R_Distance_6'],axis=1)\n",
    "df_nachbarn.to_csv('./Nearest_Neighbor_brute.csv',index=False, sep=',',header= 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Types bearbeiten\n",
    "Convert_dic= {'Movie_Id': 'int32', 'R_Movie_Id_1': 'int32','R_Distance_1':'float','R_Movie_Id_2': 'int32','R_Distance_2':'float','R_Movie_Id_3': 'int32','R_Distance_3':'float','R_Movie_Id_4': 'int32','R_Distance_4':'float','R_Movie_Id_5': 'int32','R_Distance_5':'float', 'R_Movie_Id_6': 'int32','R_Distance_6':'float'}\n",
    "df_recommendation = df_recommendation.astype(Convert_dic)\n",
    "\n",
    "# Exportieren der Nächsten Nachbarn\n",
    "df_nachbarn =df_recommendation.drop(['R_Distance_1', 'R_Distance_2', 'R_Distance_3', 'R_Distance_4', 'R_Distance_5', 'R_Distance_6'], axis= 1)\n",
    "df_nachbarn.to_csv('./Nearest_Neighbor_brute.csv',index= False, sep=',',header='True')\n",
    "\n",
    "# Exportieren der Nächsten Nachbarn davon die Distanz\n",
    "df_distance =df_recommendation.drop(['R_Movie_Id_1', 'R_Movie_Id_2', 'R_Movie_Id_3', 'R_Movie_Id_4', 'R_Movie_Id_5', 'R_Movie_Id_6'], axis= 1)\n",
    "df_distance.to_csv('./Nearest_Neighbor_distance_brute.csv',index= False, sep=',',header='True')\n",
    "\n",
    "# Export der Empfehlungen und IDs mit Rundungen als CSV\n",
    "df_rounded = df_recommendation.round({'R_Distance_1': 4, 'R_Distance_2':4,'R_Distance_3': 4, 'R_Distance_4':4,'R_Distance_5':4,'R_Distance_6':4 })\n",
    "df_rounded.to_csv('./Movie_Recommendations_header_IDs_rounded_brute.csv',index= False, sep=';',header='True', decimal=',')\n",
    "\n",
    "# Mappen der Movie_IDs and die Datei Movie_Titles\n",
    "\n",
    "df_rounded['Movie_Id'] = df_rounded['Movie_Id'].map(df_movie_titles.set_index('Movie_Id')['Name'])\n",
    "df_rounded['R_Movie_Id_1'] = df_rounded['R_Movie_Id_1'].map(df_movie_titles.set_index('Movie_Id')['Name'])\n",
    "df_rounded['R_Movie_Id_2'] = df_rounded['R_Movie_Id_2'].map(df_movie_titles.set_index('Movie_Id')['Name'])\n",
    "df_rounded['R_Movie_Id_3'] = df_rounded['R_Movie_Id_3'].map(df_movie_titles.set_index('Movie_Id')['Name'])\n",
    "df_rounded['R_Movie_Id_4'] = df_rounded['R_Movie_Id_4'].map(df_movie_titles.set_index('Movie_Id')['Name'])\n",
    "df_rounded['R_Movie_Id_5'] = df_rounded['R_Movie_Id_5'].map(df_movie_titles.set_index('Movie_Id')['Name'])\n",
    "df_rounded['R_Movie_Id_6'] = df_rounded['R_Movie_Id_6'].map(df_movie_titles.set_index('Movie_Id')['Name'])\n",
    "\n",
    "# Exportieren der Empfehlungen mit Filmnamen\n",
    "df_rounded.to_csv('/Movie_Recommendations_header_Titels_rounded.csv',index= False, sep=';',header='True', decimal=',')\n",
    "\n",
    "#löschen der Variable\n",
    "\n",
    "del df_rounded, df_nachbarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  5.3 Vorbereitung für Precision and Recall </a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Um die Laufzeit zu verbessern, werden mit dieser Funktion die zuvor ausgegebenen Distances und Nearest Neighbor mit Hilfe einer CSV eingelesen\n",
    "\n",
    "def empfehlung_movie (best_movie):\n",
    "    # Leere arrays erstellen\n",
    "    nachbarn = []\n",
    "    index = best_movie\n",
    "\n",
    "    df_nachbarn = pd.read_csv('./Nearest_Neighbor+1.csv')\n",
    "    l_nachbarn = df_nachbarn.iloc[index-1]\n",
    "    l_nachbarn = list(l_nachbarn.tolist())[:0:-1]\n",
    "    return l_nachbarn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  5.4 Precision and Recall </a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[1]# Import Modules\n",
    "import time\n",
    "# Runtime variable\n",
    "start = time.time()\n",
    "\n",
    "### Ausklammern wenn oben bereits iteriert wurde\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from statistics import mean\n",
    "#### \n",
    "\n",
    "#Final_Code_Abgabe import empfehlung_movie --> Wenn das als Py Datei ausgeführt werden soll\n",
    "\n",
    "def get_user_ids_to_drop():\n",
    "    user_ids_to_drop = []\n",
    "    f = open('./testset.json')\n",
    "    data = json.load(f)\n",
    "    for i in data:\n",
    "        user_ids_to_drop.append(i['User_Id'])\n",
    "    return user_ids_to_drop\n",
    "\n",
    "def get_calculation_base(raw_true, raw_pred):\n",
    "    boolean_true = []\n",
    "    boolean_pred = []\n",
    "    for i in range(17770):\n",
    "        boolean_true.append(False)\n",
    "        boolean_pred.append(False)\n",
    "    for i in raw_true:\n",
    "        boolean_true[i-1] = True\n",
    "    for i in raw_pred:\n",
    "        boolean_pred[int(i-1)] = True\n",
    "    return boolean_true, boolean_pred\n",
    "\n",
    "def get_mean_precision_recall(): \n",
    "    # Opening JSON file\n",
    "    f = open('./testset.json')\n",
    "    \n",
    "    # returns JSON object as a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "    # Iterating through the dictionary\n",
    "    precision_total = []\n",
    "    recall_total = []\n",
    "    for i in data:\n",
    "        #################################################################\n",
    "        # Get predictions for the prediction base\n",
    "        raw_pred = []\n",
    "        for j in i['Prediction_Base']:\n",
    "            raw_pred = np.append(raw_pred, empfehlung_movie(j))\n",
    "        #################################################################\n",
    "        # Get true values for the prediction base\n",
    "        raw_true = i['Raw_true']\n",
    "\n",
    "        # Get precision and recall for particular testdata\n",
    "        boolean_true, boolean_pred = get_calculation_base(raw_true, raw_pred)\n",
    "        precision = precision_score(y_true = boolean_true, y_pred = boolean_pred)\n",
    "        recall = recall_score(y_true = boolean_true, y_pred = boolean_pred)\n",
    "        precision_total = np.append(precision_total,precision)\n",
    "        recall_total = np.append(recall_total,recall)\n",
    "\n",
    "    return mean(precision_total), mean(recall_total)\n",
    "\n",
    "mean_precision, mean_recall = get_mean_precision_recall()\n",
    "print(mean_precision)\n",
    "print(mean_recall)\n",
    "\n",
    "[8]# Runtime analysis\n",
    "end = time.time()\n",
    "print('Runtime: {:5.3f}s'.format(end-start))\n",
    "\n",
    "#Löschen von Variablen\n",
    "del end, start, mean_precision, mean_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  5.5. Empfehlung mit Hilfe des Fuzzy Algorithmus </a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen einer Mappers von IDs zu Filmtitel\n",
    "movie_to_idx = {\n",
    "    movie: i for i, movie in \n",
    "    enumerate(list(df_movie_titles.set_index('Movie_Id').loc[movie_user_mat.index].Name))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_matching(mapper, fav_movie, verbose=True):\n",
    "    \"\"\"\n",
    "    gibt die nächstliegende Übereinstimmung per Fuzzy zurück. Wenn keine Übereinstimmung gefunden wird, wird None zurückgegeben.\n",
    "\n",
    "    Rückgabe\n",
    "    ------\n",
    "    Index der nächstgelegenen Übereinstimmung\n",
    "    \"\"\"\n",
    "    match_tuple = []\n",
    "    # Suchen vom Match\n",
    "    for title, idx in mapper.items():\n",
    "        ratio = fuzz.ratio(title.lower(), fav_movie.lower())\n",
    "        if ratio >= 60:\n",
    "            match_tuple.append((title, idx, ratio))\n",
    "    # Sortieren der Matches\n",
    "    match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1]\n",
    "    if not match_tuple:\n",
    "        print('Kein Film gefunden')\n",
    "        return\n",
    "    if verbose:\n",
    "        print('Folgende Filme wurden in der Datenbank gefunden: {0}\\n'.format([x[0] for x in match_tuple]))\n",
    "    return match_tuple[0][1]\n",
    "\n",
    "\n",
    "\n",
    "def make_recommendation(model_knn, data, mapper, fav_movie, n_recommendations):\n",
    "    # Fitting\n",
    "    model_knn.fit(data)\n",
    "    # Ausgabe des Index\n",
    "    print('Deine Eingabe war:', fav_movie)\n",
    "    idx = fuzzy_matching(mapper, fav_movie, verbose=True)\n",
    "    distances, indices = model_knn.kneighbors(data[idx], n_neighbors=n_recommendations+1)\n",
    "    # Holen der Index -- Film Liste\n",
    "    raw_recommends = \\\n",
    "        sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())), key=lambda x: x[1])[:0:-1]\n",
    "    # Reversives Mapping\n",
    "    reverse_mapper = {v: k for k, v in mapper.items()}\n",
    "    # Ausgabe\n",
    "    print('Empfehlung für {}:'.format(fav_movie))\n",
    "    for i, (idx, dist) in enumerate(raw_recommends):\n",
    "        print('{0}: {1}, mit Distanz auf {2}'.format(i+1, reverse_mapper[idx], dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_favorite = '  <<Type Your Movie Name >>'\n",
    "\n",
    "make_recommendation(\n",
    "    model_knn=model_knn,\n",
    "    data=user_movie_table_matrix,\n",
    "    fav_movie=my_favorite,\n",
    "    mapper=movie_to_idx,\n",
    "    n_recommendations=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1acf06c8fd8bc80800f08a72a7dc06ca6ff72dd3d390f63a44d70f4da57fac27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
