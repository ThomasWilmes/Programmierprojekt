{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy Methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install  Packets\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install lightfm\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install surprise\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install scikit-surprise \n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install google.colab\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install plotly\n",
    "!{sys.executable} -m pip install fastparquet\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install pylance\n",
    "\n",
    "#Only Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To store the data\n",
    "import pandas as pd\n",
    "\n",
    "# To do linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# To create plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To create interactive plots\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# To shift lists\n",
    "from collections import deque\n",
    "\n",
    "# To compute similarities between vectors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# To use recommender systems\n",
    "import surprise as sp\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "\n",
    "# To create deep learning models\n",
    "from keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# To create sparse matrices\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# To light fm\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "# To stack sparse matrices\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# TO Fuzz\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# \n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
    "                                                     recall_at_k, get_top_k_items)\n",
    "from recommenders.models.surprise.surprise_utils import predict, compute_ranking_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Id</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17765</th>\n",
       "      <td>17766</td>\n",
       "      <td>Where the Wild Things Are and Other Maurice Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17766</th>\n",
       "      <td>17767</td>\n",
       "      <td>Fidel Castro: American Experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17767</th>\n",
       "      <td>17768</td>\n",
       "      <td>Epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17768</th>\n",
       "      <td>17769</td>\n",
       "      <td>The Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17769</th>\n",
       "      <td>17770</td>\n",
       "      <td>Alien Hunter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17770 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Movie_Id                                               Name\n",
       "0             1                                    Dinosaur Planet\n",
       "1             2                         Isle of Man TT 2004 Review\n",
       "2             3                                          Character\n",
       "3             4                       Paula Abdul's Get Up & Dance\n",
       "4             5                           The Rise and Fall of ECW\n",
       "...         ...                                                ...\n",
       "17765     17766  Where the Wild Things Are and Other Maurice Se...\n",
       "17766     17767                  Fidel Castro: American Experience\n",
       "17767     17768                                              Epoch\n",
       "17768     17769                                        The Company\n",
       "17769     17770                                       Alien Hunter\n",
       "\n",
       "[17770 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Als Nächstes sollen alle Pfade mit variablen belegt werden, dass macht das austauschen einfacher.\n",
    "\n",
    "movie_tile_File = 'C:/Users/jsbreite/OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/movie_titles.csv'\n",
    "movie_tile_File_new = 'C:/Users/jsbreite/OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/Movie_Titles_clear.gzip'\n",
    "combined_data_1 = 'C:/Users/jsbreite/OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/combined_data_1.txt'\n",
    "combined_data_2 = 'C:/Users/jsbreite/OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/combined_data_2.txt'\n",
    "combined_data_3 = 'C:/Users/jsbreite/OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/combined_data_3.txt'\n",
    "combined_data_4 = 'C:/Users/jsbreite/OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/combined_data_4.txt'\n",
    "new_Combined = 'C:/Users/jsbreite/OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/Combined_Data_All.gzip'\n",
    "netflix_rating_Combined = 'C:/Users/jsbreite\\OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/netflix_data.csv'\n",
    "parquet_combined_data = 'C:/Users/jsbreite\\OneDrive - Jannis Breitenstein IT/Hochschule_Studium/5_Semester/Programmierprojekt/Netflix_Daten/data_Comb.zip'\n",
    "\n",
    "\n",
    "# Read Parquet\n",
    "data = pd.read_parquet(new_Combined)\n",
    "Convert_dic_data= {'Cust_Id': 'int64', 'Rating': 'float32', 'Movie_Id': 'Int64'}\n",
    "data= data.astype(Convert_dic_data)\n",
    "\n",
    "# Reorder Columns\n",
    "del new_Combined, Convert_dic_data\n",
    "data = data[['Cust_Id', 'Movie_Id', 'Rating']]\n",
    "## Read Movie Data\n",
    "movie_titles = pd.read_csv(movie_tile_File,\n",
    "                           encoding = \"ISO-8859-1\",\n",
    "                           delimiter= '\\t',\n",
    "                           header = None,\n",
    "                           names = ['Target'])\n",
    "                           # Speicher alle Daten erst in eine Reihe, danach trennt er diese\n",
    "movie_titles[['Movie_Id', 'Year', 'Name']] = movie_titles['Target'].str.split(pat=\",\",n=2, expand=True)   \n",
    "movie_titles= movie_titles.drop(['Target', 'Year'], axis= 1)  \n",
    "Convert_dic= {'Movie_Id': 'int64', 'Name': 'str'}\n",
    "movie_titles = movie_titles.astype(Convert_dic)\n",
    "#movie_titles.astype({'Movie_Id': 'int64', 'Name': 'str'}).dtypes\n",
    "#movie_titles.rename(columns={'Id':'Movie_Id'}, inplace=True)\n",
    "#movie_titles = movie_titles.reset_index(drop=True)\n",
    "display(movie_titles)\n",
    "del movie_tile_File, Convert_dic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape User-Ratings unfiltered:\t(100480507, 3)\n",
      "Shape User-Ratings filtered:\t(75010415, 3)\n"
     ]
    }
   ],
   "source": [
    "df = data\n",
    "# Filter sparse movies\n",
    "min_movie_ratings = 1000\n",
    "filter_movies = (df['Movie_Id'].value_counts()>min_movie_ratings)\n",
    "filter_movies = filter_movies[filter_movies].index.tolist()\n",
    "\n",
    "# Filter sparse users\n",
    "min_user_ratings = 200\n",
    "filter_users = (df['Cust_Id'].value_counts()>min_user_ratings)\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "# Actual filtering\n",
    "df_filterd = df[(df['Movie_Id'].isin(filter_movies)) & (df['Cust_Id'].isin(filter_users))]\n",
    "del filter_movies, filter_users, min_movie_ratings, min_user_ratings, data\n",
    "\n",
    "#Reorder the Columns\n",
    "df_filterd = df_filterd[['Cust_Id', 'Movie_Id', 'Rating']]\n",
    "\n",
    "print('Shape User-Ratings unfiltered:\\t{}'.format(df.shape))\n",
    "print('Shape User-Ratings filtered:\\t{}'.format(df_filterd.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapper from movie title to index\n",
    "# pivot and create movie-user matrix\n",
    "movie_user_mat = df_filterd.pivot(index='Movie_Id', columns='Cust_Id', values='Rating').fillna(0)\n",
    "\n",
    "# create mapper from movie title to index\n",
    "movie_to_idx = {\n",
    "    movie: i for i, movie in \n",
    "    enumerate(list(movie_titles.set_index('Movie_Id').loc[movie_user_mat.index].Name))\n",
    "}\n",
    "# transform matrix to scipy sparse matrix\n",
    "# movie_user_mat_sparse = csr_matrix(movie_user_mat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(metric='cosine', n_jobs=-1, n_neighbors=120)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='auto', n_neighbors=120, n_jobs=-1)\n",
    "# fit\n",
    "model_knn.fit(movie_user_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_matching(mapper, fav_movie, verbose=True):\n",
    "    #https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system\n",
    "    #https://www.kaggle.com/code/razor08/knn-based-collaborative-filtering?scriptVersionId=81850169  Wo kommmt der Code her\n",
    "    \"\"\"\n",
    "    return the closest match via fuzzy ratio. If no match found, return None\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    mapper: dict, map movie title name to index of the movie in data\n",
    "\n",
    "    fav_movie: str, name of user input movie\n",
    "    \n",
    "    verbose: bool, print log if True\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    index of the closest match\n",
    "    \"\"\"\n",
    "    match_tuple = []\n",
    "    # get match\n",
    "    for title, idx in mapper.items():\n",
    "        ratio = fuzz.ratio(title.lower(), fav_movie.lower())\n",
    "        if ratio >= 60:\n",
    "            match_tuple.append((title, idx, ratio))\n",
    "    # sort\n",
    "    match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1]\n",
    "    if not match_tuple:\n",
    "        print('Oops! No match is found')\n",
    "        return\n",
    "    if verbose:\n",
    "        print('Found possible matches in our database: {0}\\n'.format([x[0] for x in match_tuple]))\n",
    "    return match_tuple[0][1]\n",
    "\n",
    "\n",
    "\n",
    "def make_recommendation(model_knn, data, mapper, fav_movie, n_recommendations):\n",
    "    # fit\n",
    "    model_knn.fit(data)\n",
    "    # get input movie index\n",
    "    print('You have input movie:', fav_movie)\n",
    "    idx = fuzzy_matching(mapper, fav_movie, verbose=True)\n",
    "    # inference\n",
    "    print('Recommendation system start to make inference')\n",
    "    print('......\\n')\n",
    "    distances, indices = model_knn.kneighbors(data[idx], n_neighbors=n_recommendations+1)\n",
    "    # get list of raw idx of recommendations\n",
    "    raw_recommends = \\\n",
    "        sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())), key=lambda x: x[1])[:0:-1]\n",
    "    # get reverse mapper\n",
    "    reverse_mapper = {v: k for k, v in mapper.items()}\n",
    "    # print recommendations\n",
    "    print('Recommendations for {}:'.format(fav_movie))\n",
    "    for i, (idx, dist) in enumerate(raw_recommends):\n",
    "        print('{0}: {1}, with distance of {2}'.format(i+1, reverse_mapper[idx], dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_favorite = 'Haiku Tunnel'\n",
    "\n",
    "make_recommendation(\n",
    "    model_knn=model_knn,\n",
    "    data=movie_user_mat,\n",
    "    fav_movie=my_favorite,\n",
    "    mapper=movie_to_idx,\n",
    "    n_recommendations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Reader class is used to parse a file containing ratings.\n",
    "# The file is assumed to specify only one rating per line, such as in the df_ratings_temp file above.\n",
    "reader = Reader()\n",
    "ratings_by_users = Dataset.load_from_df(df_filterd[['Cust_Id', 'Movie_Id','Rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Könnte interessant sein\n",
    "https://github.com/microsoft/recommenders/blob/main/examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb\n",
    "https://www.relataly.com/building-a-movie-recommender-using-collaborative-filtering/4376/#h-step-3-split-the-data-in-train-and-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into train and test\n",
    "train_df, test_df = train_test_split(ratings_by_users, test_size=.4)\n",
    "#algo = BaselineOnly(bsl_options)\n",
    "#predictions = algo.fit(trainset).test(testset)\n",
    "#accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an SVD model\n",
    "svd_model = SVD()\n",
    "svd_model_trained = svd_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:822\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[0;32m    824\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    825\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    829\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jsbreite\\OneDrive - Jannis Breitenstein IT\\Hochschule_Studium\\5_Semester\\Programmierprojekt\\Codes\\Github_V1\\Best_of_V1_Cross-Validation_Fuzzy.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jsbreite/OneDrive%20-%20Jannis%20Breitenstein%20IT/Hochschule_Studium/5_Semester/Programmierprojekt/Codes/Github_V1/Best_of_V1_Cross-Validation_Fuzzy.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 10-fold cross validation \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jsbreite/OneDrive%20-%20Jannis%20Breitenstein%20IT/Hochschule_Studium/5_Semester/Programmierprojekt/Codes/Github_V1/Best_of_V1_Cross-Validation_Fuzzy.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cross_val_results \u001b[39m=\u001b[39m cross_validate(svd_model_trained, ratings_by_users, measures\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mRMSE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mMSE\u001b[39;49m\u001b[39m'\u001b[39;49m], cv\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jsbreite/OneDrive%20-%20Jannis%20Breitenstein%20IT/Hochschule_Studium/5_Semester/Programmierprojekt/Codes/Github_V1/Best_of_V1_Cross-Validation_Fuzzy.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_mae \u001b[39m=\u001b[39m cross_val_results[\u001b[39m'\u001b[39m\u001b[39mtest_mae\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jsbreite/OneDrive%20-%20Jannis%20Breitenstein%20IT/Hochschule_Studium/5_Semester/Programmierprojekt/Codes/Github_V1/Best_of_V1_Cross-Validation_Fuzzy.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# mean squared errors per fold\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\site-packages\\surprise\\model_selection\\validation.py:108\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(algo, data, measures, cv, return_train_measures, n_jobs, pre_dispatch, verbose)\u001b[0m\n\u001b[0;32m    102\u001b[0m cv \u001b[39m=\u001b[39m get_cv(cv)\n\u001b[0;32m    104\u001b[0m delayed_list \u001b[39m=\u001b[39m (\n\u001b[0;32m    105\u001b[0m     delayed(fit_and_score)(algo, trainset, testset, measures, return_train_measures)\n\u001b[0;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m (trainset, testset) \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(data)\n\u001b[0;32m    107\u001b[0m )\n\u001b[1;32m--> 108\u001b[0m out \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch)(delayed_list)\n\u001b[0;32m    110\u001b[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mout)\n\u001b[0;32m    112\u001b[0m test_measures \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py:833\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    830\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    831\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[1;32m--> 833\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[0;32m    834\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\site-packages\\surprise\\model_selection\\validation.py:104\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    100\u001b[0m measures \u001b[39m=\u001b[39m [m\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m measures]\n\u001b[0;32m    102\u001b[0m cv \u001b[39m=\u001b[39m get_cv(cv)\n\u001b[1;32m--> 104\u001b[0m delayed_list \u001b[39m=\u001b[39m (\n\u001b[0;32m    105\u001b[0m     delayed(fit_and_score)(algo, trainset, testset, measures, return_train_measures)\n\u001b[0;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m (trainset, testset) \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(data)\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    108\u001b[0m out \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)(delayed_list)\n\u001b[0;32m    110\u001b[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mout)\n",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\site-packages\\surprise\\model_selection\\split.py:117\u001b[0m, in \u001b[0;36mKFold.split\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    112\u001b[0m raw_trainset \u001b[39m=\u001b[39m [\n\u001b[0;32m    113\u001b[0m     data\u001b[39m.\u001b[39mraw_ratings[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m chain(indices[:start], indices[stop:])\n\u001b[0;32m    114\u001b[0m ]\n\u001b[0;32m    115\u001b[0m raw_testset \u001b[39m=\u001b[39m [data\u001b[39m.\u001b[39mraw_ratings[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices[start:stop]]\n\u001b[1;32m--> 117\u001b[0m trainset \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mconstruct_trainset(raw_trainset)\n\u001b[0;32m    118\u001b[0m testset \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mconstruct_testset(raw_testset)\n\u001b[0;32m    120\u001b[0m \u001b[39myield\u001b[39;00m trainset, testset\n",
      "File \u001b[1;32mc:\\Users\\jsbreite\\Anaconda3\\lib\\site-packages\\surprise\\dataset.py:206\u001b[0m, in \u001b[0;36mDataset.construct_trainset\u001b[1;34m(self, raw_trainset)\u001b[0m\n\u001b[0;32m    203\u001b[0m         raw2inner_id_items[irid] \u001b[39m=\u001b[39m current_i_index\n\u001b[0;32m    204\u001b[0m         current_i_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 206\u001b[0m     ur[uid]\u001b[39m.\u001b[39mappend((iid, r))\n\u001b[0;32m    207\u001b[0m     ir[iid]\u001b[39m.\u001b[39mappend((uid, r))\n\u001b[0;32m    209\u001b[0m n_users \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(ur)  \u001b[39m# number of users\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation \n",
    "cross_val_results = cross_validate(svd_model_trained, ratings_by_users, measures=['RMSE', 'MAE', 'MSE'], cv=10, verbose=False)\n",
    "test_mae = cross_val_results['test_mae']\n",
    "\n",
    "# mean squared errors per fold\n",
    "df_test_mae = pd.DataFrame(test_mae, columns=['Mean Absolute Error'])\n",
    "df_test_mae.index = np.arange(1, len(df_test_mae) + 1)\n",
    "df_test_mae.sort_values(by='Mean Absolute Error', ascending=False).head(15)\n",
    "\n",
    "# plot an overview of the performance per fold\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.barplot(y='Mean Absolute Error', x=df_test_mae.index, data=df_test_mae, color=\"b\")\n",
    "# plt.title('Mean Absolute Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict ratings for a single user_id and for all movies\n",
    "user_id = 400 # some test user from the ratings file\n",
    "\n",
    "# create the predictions\n",
    "pred_series= []\n",
    "df_ratings_filtered = df_filterd[df_filterd['Cust_Id'] == user_id]\n",
    "\n",
    "print(f'number of ratings: {df_ratings_filtered.shape[0]}')\n",
    "for movie_id, name in zip(movie_titles.index, movie_titles['Name']):\n",
    "    # check if the user has already rated a specific movie from the list\n",
    "    rating_real = df_filterd.query(f'Movie_Id == {movie_id}')['Rating'].values[0] if movie_id in df_ratings_filtered['Movie_Id'].values else 0\n",
    "    # generate the prediction\n",
    "    rating_pred = svd_model_trained.predict(user_id, movie_id, rating_real, verbose=False)\n",
    "    # add the prediction to the list of predictions\n",
    "    pred_series.append([movie_id, name, rating_pred.est, rating_real])\n",
    "\n",
    "# print the results\n",
    "df_recommendations = pd.DataFrame(pred_series, columns=['Movie_Id', 'Name', 'predicted_rating', 'actual_rating'])\n",
    "df_recommendations.sort_values(by='predicted_rating', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict ratings for the combination of user_id and movie_id\n",
    "user_id = 217 # some test user from the ratings file\n",
    "movie_id = 4002\n",
    "rating_real = df_filterd.query(f'Movie_Id == {movie_id} & Cust_Id == {user_id}')['Rating'].values[0]\n",
    "movie_title = movie_titles[movie_titles.index == 862]['Name'].values[0]\n",
    "\n",
    "print(f'Movie title: {movie_title}')\n",
    "print(f'Actual rating: {rating_real}')\n",
    "\n",
    "# predict and show the result\n",
    "rating_pred = svd_model_trained.predict(user_id, movie_id, rating_real, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andere Methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_random_split(df_filterd, 0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1acf06c8fd8bc80800f08a72a7dc06ca6ff72dd3d390f63a44d70f4da57fac27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
